# Post Implementation
For complete information, practical guides and resources on post implementation please visit our website(https://www.paritii.com/section/post-implementation).

In the post-implementation phase, the focus shifts to ensuring that the deployed AI system continues to uphold principles of equity, fairness, transparency, and inclusivity prioritized during development. Recognizing the dynamic nature of artificial intelligence and its potential impact on diverse communities, this stage plays a pivotal role in mitigating biases, fostering AI awareness and literacy, and addressing the evolving needs of users.  From a technical perspective, this phase includes implementing regular audits, performance evaluations, and continuous monitoring to assess the AI system's adherence to ethical guidelines and equitable outcomes.

For complete information, practical guides and resources on post implementation please visit our website(https://www.paritii.com/section/post-implementation).

## AI Literacy, Education, and Awareness Building
AI literacy and end-user education promote the explainability of algorithms by increasing stakeholders' understanding of AI concepts and empowering end-users to engage with AI systems effectively. 
### Model Explainability
Model explainability is specific to the AI system and refers to the ability to provide transparent insights into its decision-making processes, enabling stakeholders, including end-users, to understand and interpret the rationale behind its outputs. Explainability makes AI decisions understandable to individuals who may not have technical expertise, and developers can achieve it in several ways:
* Global Explanations: Provide insights into the overall behavior of the model by analyzing feature importance, model parameters, or feature contributions across the entire dataset.
* Interpretable Models: Use models that inherently provide transparency, such as decision trees, linear regression, or rule-based systems. These models offer clear rules or features that explain their predictions.
* Local Explanations: Techniques like LIME (Local Interpretable Model-agnostic Explanations) or SHAP (SHapley Additive exPlanations) generate explanations for individual predictions, making it easier to understand why a specific decision was made.[^1]:
 [^1]: Athar, A. (2020, October 4). SHAP (SHapley Additive exPlanations) And LIME (Local Interpretable Model-agnostic Explanations) for model explainability. Medium; Analytics Vidhya. https://medium.com/analytics-vidhya/shap-shapley-additive-explanations-and-lime-local-interpretable-model-agnostic-explanations-8c0aa33e91f 
* Simplification: Simplify complex models using techniques like feature selection, dimensionality reduction, or model distillation, which create more interpretable versions of the original model while preserving its performance.
* Post-hoc Explanations: Utilize methods like feature importance scores, partial dependence plots, or sensitivity analysis to explain the model's predictions after it has been trained.
* Interactive Visualization: Present model explanations through interactive visualizations that allow users to intuitively explore how different features affect predictions and understand the decision-making process.
* Natural Language Generation: Automatically generate human-readable explanations in natural language to describe the reasoning behind model predictions clearly and understandably.
* Ethical Considerations: Consider ethical implications and fairness metrics when designing and explaining AI models, ensuring that explanations reflect the model's adherence to fairness principles and ethical guidelines.
* Documentation and Communication: Provide comprehensive documentation and communication channels to explain the model's purpose, limitations, and potential biases to stakeholders, promoting trust and transparency in its use.

AI literacy and end-user education complement each other in promoting the explainability of algorithms by increasing stakeholders' understanding of AI concepts and empowering end-users to engage with AI systems effectively. A higher level of AI literacy among stakeholders, including administrators, faculty, staff, students, and other stakeholders at an institution, is fundamental to effective solution implementation. Part of planning for successfully implementing an AI solution involves defining a strategy for supporting various stakeholder groups in using the systems and outputs to align with its equitable use case. 

For complete information, practical guides and resources on AI Literacy, Education, and Awareness Building, please visit our website(https://www.paritii.com/detail/ai-literacy-education-awareness-building).
## Customer Feedback and Impact
User Experience(UX) testing is a comprehensive approach to evaluating a user's experience with a digital product or service at every touchpoint, focusing on their perception of its performance, emotional response, perceived value, and overall satisfaction. It involves real or representative users evaluating a product by attempting to complete tasks without prior knowledge. Collecting customer feedback during UX testing is a necessary step in developing and implementing equitable AI solutions, because it empowers leaders with the information needed to identify and address bias and barriers to access for a diverse user base. 

Once user testing has been conducted, a range of key usability metrics come into play, serving as critical indicators to unlock valuable user insights. These metrics provide a robust framework for understanding user interactions, satisfaction, and product effectiveness and efficiency, highlighting areas of strength and pinpointing opportunities for improvement.
* Completion Rate: Measures the percentage of users who complete a given task. A high completion rate indicates that a website or application is easy to use and navigate.
* Error Rate: Tracks the number of errors users make while performing tasks. This metric helps identify areas of a product that may be confusing or difficult to use.
* Time on Task: Evaluates the amount of time users spend completing a specific task. Shorter times typically suggest better usability, although some tasks may require more in-depth engagement.
* User Satisfaction: Assessed through surveys and feedback forms, this metric provides direct insight into how users feel about their experience with the product.
* Task Success Rate: this metric focuses on whether users can successfully achieve their goals within the application or website.
* Clicks to Completion: Measures how many clicks or interactions it takes for a user to complete a task. Fewer clicks generally indicate a more efficient, user-friendly design.
* Usability Test Scores: Derived from structured usability tests, these scores compile various aspects of user interactions and satisfaction into a comprehensive assessment of the productâ€™s usability.
* Net Promoter Score (NPS): A key indicator of customer loyalty, NPS measures how likely users are to recommend the product or service to others. It categorizes respondents into Promoters, Passives, and Detractors based on their likelihood to recommend.
* Customer Satisfaction (CSAT): This KPI gauges user satisfaction with a product or service at specific touchpoints or after particular interactions.
* Conversion Rate: Tracks the percentage of users who take a desired action, such as making a purchase or signing up for a newsletter, which can indicate the effectiveness of the UX design in facilitating user goals.
* Retention Rate: Measures how many users continue to use the product over time, reflecting the long-term value and usability of the product.
* Task Load Index (NASA-TLX): A more specialized metric, the Task Load Index assesses the perceived workload required to use the product, considering factors like effort and frustration.

The UX and customer feedback research constitute an iterative process, not one-time actions. Establishing a continuous feedback loop post-development will provide insights and customer inputs as developers refine their products. 

For complete information, practical guides and resources on customer feedback and impact, please visit our website(https://www.paritii.com/detail/customer-feedback-impact).
## Post Launch Assessment Guide and Strategic Decisions for Ongoing Use
Post-launch assessments are essential for maintaining fairness, transparency, and accountability and ensuring that products and services remain inclusive and responsive to the diverse needs and preferences of all users. Organizations can identify and address any potential biases or disparities that may arise after launch by conducting iterative assessments, thereby promoting fairness and equity in the user experience. 
### Ongoing Evaluation
Performance evaluation is a set of measures to assess the effectiveness, efficiency, and reliability of machine learning models.[^2]:
[^2]: Srivastava, T. (2019, August 6). 12 Important Model Evaluation Metrics for Machine Learning Everyone Should Know (Updated 2023). Analytics Vidhya. https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/
The following measures, when implemented, can ensure a robust approach to gauging the effectiveness of machine learning model development and adoption.
* Model Metrics:
Model metrics are quantitative measures used to evaluate the performance of machine learning models. These metrics provide insights into how well a model is performing and can help assess its effectiveness for a given task. It is important to evaluate model performance metrics such as accuracy, precision, recall, F1 score, and area under the ROC curve (AUC), and assess performance on both training and test/validation datasets.
* Real-World Performance:
Real-world performance refers to the effectiveness, reliability, and impact of a machine learning model when deployed in a production environment and applied to real-world data and tasks.
* Comparative Analysis:
A comparative analysis involves evaluating the performance, features, or characteristics of different machine learning models, algorithms, techniques, or approaches to identify strengths, weaknesses, and trade-offs.

Anomaly detection and error analysis help identify and understand model shortcomings and evaluate the performance and reliability of machine learning models, particularly in applications where identifying unusual or unexpected patterns is essential.[^3]:
[^3]:Anomaly Detection in Machine Learning. (2021). Anomaly Detection in Machine Learning. https://serokell.io/blog/anomaly-detection-in-machine-learning.
**Error analysis**  in the context of equity in AI refers to the examination and understanding of the types and sources of errors made by machine learning models, particularly concerning their impact on fairness, inclusivity, and ethical considerations. 

Model robustness and stability are essential attributes of machine learning models, ensuring that they perform reliably and consistently across different conditions, datasets, and environments. Robustness is the ability of a model to maintain performance and generalization capabilities under perturbations or variations in input data, such as noise, outliers, or adversarial attacks. Stability is the consistency of a model's predictions and behavior across different datasets, subsets of data, or operational conditions, reflecting its reliability and resilience to changes.

Some practices to support robustness and performance stability include:

Robustness:
* Stress Tests: Conduct stress tests to evaluate model robustness under challenging conditions, such as noisy data, outliers, or adversarial attacks.
* Noise Robustness: Assess the model's performance in the presence of noise by adding random perturbations or disturbances to input data and measuring the degradation in performance.
* Adversarial Robustness: Test the model's resilience against adversarial attacks by generating perturbed inputs that are specifically crafted to deceive the model while appearing like legitimate data.
* Outlier Robustness: Evaluate the model's ability to handle outliers or anomalous instances that deviate significantly from the normal data distribution without compromising performance.
* Regularization Techniques: Apply regularization methods such as L1/L2 regularization, dropout, or weight decay to prevent overfitting and improve generalization capabilities.
* Adversarial Training: Incorporate adversarial examples into the training process to harden the model against adversarial attacks and improve its ability to resist manipulation.
* Ensemble Methods: Ensemble multiple models with diverse architectures, initialization, or training data to improve robustness and reduce the impact of individual model vulnerabilities.

Performance Stability:
* Monitor model performance over time to ensure stability and consistency in predictions.
* Assess sensitivity to changes in input data distributions or operational conditions.
* Cross-Validation: Perform cross-validation across different folds or partitions of the dataset to assess the stability of model performance and generalization across multiple subsets.
* Domain Adaptation: Evaluate the model's stability across different domains or distributions by testing its performance on out-of-domain data or data collected from diverse sources.
* Temporal Stability: Analyze the stability of model predictions over time by monitoring performance metrics and error rates across different periods or batches of data.
* Transfer Learning: Fine-tune pre-trained models on target datasets or domains to leverage knowledge learned from related tasks and improve stability and generalization.
* Domain Adaptation Techniques: Apply domain adaptation methods such as adversarial domain adaptation, domain confusion, or domain-specific feature alignment to align distributions and enhance stability across domains.
* Dynamic Learning Rate Scheduling: Adjust learning rates dynamically during training based on performance metrics or model convergence to improve stability and prevent overfitting.

### Regulatory Compliance
Staying abreast of the evolving regulatory space around AI solutions will support the long-term success of an AI solution. Compliance with regulations ensures that data is handled appropriately, particularly in an education setting, protecting individuals' privacy rights and mitigating the risk of legal penalties.

### Documentation and Reporting
 Develop a comprehensive documentation plan outlining the types of documentation needed, the intended audience, and the documentation format. Define the scope of documentation, including model architecture, data preprocessing steps, training procedures, evaluation metrics, deployment considerations, and regulatory compliance. 
As part of  post-launch assessment, compile a comprehensive report summarizing evaluation results, findings, and recommendations. The assessment should quantify equity metrics such as disparate impact, equal opportunity, demographic parity, and other fairness measures that evaluate the fairness of model outcomes.  

The post-launch report should examine biases and discrimination in model predictions and outcomes and document the efforts undertaken to mitigate bias, discrimination, and other equity issues in machine learning models. Document any corrective actions, improvements, or enhancements implemented based on assessment outcomes. Establishing document templates, formats, and standards is recommended to ensure consistency and clarity across different documentation artifacts. In addition, create procedures and responsibilities, including reviews and audits, for maintaining and updating documentation in response to changes in project requirements, codebase modifications, or evolving best practices.
### Continuous Improvement and Iteration
Continuous improvement and iteration are essential principles for enhancing the effectiveness, efficiency, and quality of machine learning projects over time. 

For complete information, practical guides and resources on Post Launch Assessment Guide and Strategic Decisions for Ongoing Use, please visit our website(https://www.paritii.com/detail/post-launch-assessment).
